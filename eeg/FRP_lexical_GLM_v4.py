#%%
import mne
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import glob
import os
import meegkit
from mne.datasets import sample
# from mne.stats.regression import linear_regression_raw
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
import scipy
from functools import partial
from mne_custom_regression import ridge_regression_raw
import seaborn as sns
sns.set_palette("tab10")
import traceback
# get current dir to construct relative paths
dir_path = os.path.dirname(os.path.realpath(__file__))
os.chdir(dir_path)
pd.options.mode.chained_assignment = None  # default='warn'
plt.ioff()

#%% paths
dir_raw = '/Volumes/Blue1TB/EyeMindLink/Data'
dir_fif = '/Volumes/Blue1TB/EEG_processed/preprocessed_fif/'
event_fn_suffix = '_eyetracker_events.csv' # inside dir_fif, events containing fixations sacs and blinks as well as tasks, generated by merge_eyetracker_eeg.py
dir_out_par = '/Volumes/Blue1TB/EEG_processed/'
REDO = True
FORCE_REDO = False
channels = ['CPz', 'FCz', 'AFF5h', 'AFF6h', 'CCP5h', 'CCP6h', 'PPO9h', 'PPO10h']
decimation = 1 #TODO: set to 1 for final pass analysis
ridge_alpha = 1
verstr = f'FRP_TRF_lexical_v4_alpha{ridge_alpha}_decim{decimation}'
dir_out = os.path.join(dir_out_par, verstr)
# v3: after discussion on 2025-01-31

# exclude subjects:
# - with MW % <20% or >80%
# - with overall comprehension score not different from chance
# - with fewer than 60 fixations per condition (MW/non-MW) after filtering as below

# stricter prepro: filter 0.5-15Hz, downsample to 100 Hz, reject windows with >120uV peak tp peak range
# infill covariates after scaling with 0 not mean

# only include the following fixation onsets:
# - on an IA
# - on content words
# - 50-1000 ms duration
# - inbound saccade amplitude within 3 stdev of mean

# this version uses dummy coding for MW: intercept is FRP, MW=-1 and MW=0 are additional predictors
# there will be separate columsn for each MW and lexical covariate
# separate models for blink and screen change events too to remove them #TODO:

os.makedirs(dir_out, exist_ok=True)
dir_events = os.path.expanduser('~/Emotive Computing Dropbox/Rosy Southwell/EyeMindLink/Processed/events/') # task events
ia_df = pd.read_csv('../info/ia_label_mapping_opt_surprisal.csv').rename(columns={'gpt2_surprisal_page':'surprisal'})
# remove punctuation from IA_ID
ia_df = ia_df.loc[~ia_df['punctuation']]
ia_df['IA_ID'] = ia_df['IA_ID'].fillna('-1').astype(float).astype(int)
ia_df['log_word_freq'] = ia_df['word_freq'].astype(float).apply(np.log).replace(-np.inf, np.nan)

beh_df = pd.read_csv('~/Emotive Computing Dropbox/Rosy Southwell/EyeMindLink/Processed/Behaviour/EML1_page_level.csv') # comp and MW scores
eeg_trigger_df = pd.read_csv('../info/EEGtriggerSources.csv')
fn_base = '_p.fif'

pIDs = [re.findall(r'EML1_\d{3}', f)[0] for f in os.listdir(dir_fif) if f.endswith(fn_base)]
# unique and sort
pIDs = sorted(list(set(pIDs)))
exclude = [20, 21, 22, 23, 24, 25, 26, 27, 30, 31, 39, 40, 73, 77, 78, 87,88,93,99, 
    110,115,123,125, 138, 160, 164,167,168, 171,172,173, 178,179] # ubj to exclude because no eeg or no trigger etc.
skip_reasons = {} if REDO else pd.read_csv(os.path.join(dir_out, 'skip_reasons.csv'), index_col=0).to_dict()['0']
skip_reasons = {f'EML1_{pID:03d}' : 'no eeg or no IAs' for pID in exclude}
pIDs = [p for p in pIDs if int(re.findall(r'\d{3}', p)[0]) not in exclude]

# #####

# pIDs = ['EML1_050', 'EML1_084', 'EML1_106', 'EML1_114', 'EML1_118', 'EML1_121', 'EML1_131', 'EML1_141', 'EML1_144', 'EML1_148', 'EML1_169', 'EML1_175']
# resume_from = 'EML1_050'
# pIDs = pIDs[pIDs.index(resume_from):]
# #####

# %% wrap solver in a function to make it a callable
def ridge_solver(X, y, alpha=1): 
    res = Ridge(solver='auto',alpha=alpha).fit(X, y).coef_ #TODO: what else comes from Ridge
    if len(res.shape)==1:
        res = np.expand_dims(res, axis=0)
    return res
def ridge_model(X, y, solver='auto', alpha=1):
    res = Ridge(solver=solver,alpha=alpha).fit(X, y)
    return res
# plot sig ranges
def plot_with_stats(evk, pvals, channel='CPz', ax=None, alpha=0.05):
    erp_times = evk.times
    condname =  evk.comment
    channels = evk.ch_names
    if isinstance(channel, str):
        chan_ix = channels.index(channel)
    else:
        chan_ix = channel
    if ax is None:
        fig, ax = plt.subplots(1,1, figsize=(12, 6))
    evk.plot(picks=channels[chan_ix], axes=ax)
    # make bool array for if p<0.05
    sig = pvals[chan_ix] < alpha
    if np.any(sig):
        # consolidate ranges: find changepoints in sig
        changepoints = np.where(np.diff(sig))[0]
        if sig[0]:
            changepoints = np.concatenate([[0], changepoints])
        if sig[-1]:
            changepoints = np.concatenate([changepoints, [len(sig)]])
        assert len(changepoints) % 2 == 0
        # make list of start and end points from successive pairs
        ranges = np.array([[changepoints[i], changepoints[i+1]] for i in range(0,len(changepoints),2)])
        # colour significant effects with shaded background
        for i, p in enumerate(ranges):
            ax.axvspan(erp_times[p[0]], erp_times[p[1]], color='r', alpha=0.3)
    return ax

def plot_cluster(clusters, cluster_p_values, times, ax, tcfe=False): #TODO output is different for tcfe - modify plot fn
    h=None
    for i_c, c in enumerate(clusters):
        if cluster_p_values[i_c] <= 0.05:
            h = ax.axvspan(times[c[0]][0], times[c[0]][-1],color="r", alpha=0.3)
    if h:
        ax.legend((h,), ("cluster p-value < 0.05",))
    # ax.set_xlabel("time (ms)")
    # ax.set_ylabel("stat")
    return ax
from scipy.stats import ttest_rel # https://mne.discourse.group/t/mne-stats-permutation-cluster-1samp-test/3530/3
def ttest_rel_nop(*args):
    tvals, _ = ttest_rel(*args)
    return tvals

def comprehension_above_chance(x,n):
    pvals = []
    weights=[]
    for n_i in np.unique(n):
        x_i = x[n==n_i]
        p = 1/n_i
        # binomial test
        res = scipy.stats.binomtest(x_i.sum(), n=len(x_i), p=p)
        if res.pvalue > 0.05:
            print(f'performance on {n_i} alternatives is {x_i.sum()}/{len(x_i)} ({x_i.sum()/len(x_i):.2f}) which is not different from chance (p={res.pvalue:.2f})')
        else:
            print(f'performance on {n_i} alternatives is {x_i.sum()}/{len(x_i)} ({x_i.sum()/len(x_i):.2f}) which is different from chance (p={res.pvalue:.2f})')
        pvals += [res.pvalue]
        weights += [len(x_i)]
    # weighted mean p value
    res = scipy.stats.combine_pvalues(pvals, method='stouffer', weights = weights)
    return pvals, res



#%% Loop over subjects
cond_counts_all = []

if REDO:
    rERP_ALL = []
    for pID in pIDs: 
        # try:
            # check if this pID already has a result
            if os.path.exists(os.path.join(dir_out, f'{pID}_rERP-evk.fif')) and not FORCE_REDO:
                    print(f'{pID} already processed, skipping')
                    continue
            print('\n')
            print(f'Processing {pID}')
            mne.set_log_level('ERROR')
            ##### load 
            EEG = mne.io.read_raw_fif(os.path.join(dir_fif, f'{pID}_p.fif'), preload=True)
            # check all channels are present
            if set(channels).isdisjoint(EEG.ch_names):
                print(f'{pID} does not contain all channels, skipping')
                skip_reasons[pID] = 'missing channels'
                raise Exception(f'{pID} does not contain all channels, skipping')
            # check for bad channels and interpolate them
            if EEG.info['bads']:
                print(f'Interpolating bad channels {EEG.info["bads"]}')
                EEG.interpolate_bads()
            # triginfo = readtxtfile(fullfile(dir_fif, [pID '-info.txt']));
            with open(os.path.join(dir_fif, f'{pID}-info.txt')) as f:
                triginfo = f.read()
            task_events = pd.read_csv(os.path.join(dir_events, f'{pID}_events.csv')).rename(columns={'EVENT':'identifier','TrialType':'task'})
            # copy the correct EEGsample column for use depending on triginfo
            if 'LA0' in triginfo:
                task_events['eeg_sample'] = task_events['eegSD_sample_est'].astype(float).astype(int)
            else:
                task_events['eeg_sample'] = task_events['eeg_sample_est'].astype(float).astype(int)
            task_events['event_type'] = 'ButtonPress'
            # we want to model buttonpresses at the end of reading too, which are sometimes questions i.e. 'MW','SVT','Deep','Inference','Rote'
            task_events['task']=task_events['task'].fillna('other')
            task_events.loc[task_events['task'].str.contains('MW|SVT|Deep|Inference|Rote'), 'task'] = 'reading'
            task_events['latency_sec'] = task_events['eeg_sample']/EEG.info['sfreq']
            events = pd.read_csv(os.path.join(dir_fif, f'{pID}{event_fn_suffix}'))
            cols = ['event_type', 'task', 'identifier', 'description', 'eye', 'eeg_sample',
                'latency_sec', 'duration_sec', 
                'fix_pupilAvg', 'fix_pageIndex', 'IA_ID', 'IA_LABEL',
                'INBOUND_SAC_AMPLITUDE']
            task_events = task_events[[c for c in cols if c in task_events.columns]]
            events = pd.concat([events[cols], task_events])
            # treat '.' as missing in 'IA_ID'
            events['IA_ID'] = events['IA_ID'].replace('.', np.nan).fillna('-1').astype(int)
            if events['IA_ID'].nunique()==1 and events['IA_ID'].unique()[0]==-1:
                print(f'{pID} has no IA_IDs, skipping')
                skip_reasons[pID] = 'no IA_IDs'
                continue
            events['eeg_sample'] = events['eeg_sample'].astype(float).astype(int)
            # check if all events are from same eye, if not, keep only Right
            if not events['eye'].nunique()==1:
                print(f'{pID} has events from both eyes, keeping only right')
                events = events[events['eye']=='R']
            # rename all fixations as Fixation regardless of eye
            events['event_type'] = events['event_type'].replace('Fixation_R-reparsed','Fixation')
            events['event_type'] = events['event_type'].replace('Fixation_L-reparsed','Fixation')
            events['event_type'] = events['event_type'].replace('Fixation_R','Fixation')
            events['event_type'] = events['event_type'].replace('Fixation_L','Fixation')

            events['identifier'] = events['identifier'].fillna('') # some are NaN
            events['task'] = events['task'].fillna('none') # some are NaN
            events['event_type'] = events['event_type'].fillna('other') # some are NaN
            events = events.drop_duplicates(subset=['latency_sec','description'])
            beh_df_i = beh_df[beh_df['ParticipantID']==pID]
            beh_df_i['identifier'] = beh_df_i['Text'].astype(str) + (beh_df_i['PageNum']-1).astype(str)
            # merge lexical properties to events by IA_ID, which needs to be forced to be a string formatted as an integer not a float like 1.0
            events = events.merge(ia_df, how='left')
            events = events.merge(beh_df_i, how='left')
            events['task+type'] = events['task'] + '/' + events['event_type']
            # make annotations from events
            annot_all = mne.Annotations(onset=events['latency_sec'], duration=events['duration_sec'], description=events['task+type'])
            EEG.set_annotations(annot_all)

            ##### apply preprocessing to EEG (interp bads and reref alraedy done)
            EEG.filter(0.5, 15)

            ##### select events
            # select only reading 
            events = events[events['task'].str.contains('reading|sham')]
            fixations = events[events['event_type'].str.contains('Fixation') & events['task'].str.contains('reading')]
            sham_fixations = events[events['event_type'].str.contains('Fixation') & events['task'].str.contains('sham')]
            blinks = events[events['event_type'].str.contains('Blink')]
            task_events = events[events['event_type'].str.contains('ButtonPress')]
            # get sacc amp outliers for this participant as +- 3 STDEV from the mean INBOUND_SAC_AMPLITUDE
            sacc_amp_outliers = np.abs(fixations['INBOUND_SAC_AMPLITUDE'] - fixations['INBOUND_SAC_AMPLITUDE'].mean()) > 3*fixations['INBOUND_SAC_AMPLITUDE'].std()
            fixations['INBOUND_SAC_AMPLITUDE'] = fixations['INBOUND_SAC_AMPLITUDE'].fillna(0) 
            # drop fixations with extreme duration or saccade amplitude
            fixations = fixations[~sacc_amp_outliers]
            # remove fixations with duration <50 or >1000
            fixations = fixations[(fixations['duration_sec'] > 0.05) & (fixations['duration_sec'] < 1)]

            # drop fixations not on an IA 
            fixations = fixations[fixations['IA_ID']!=-1]
            # drop fixations on stop_words
            fixations = fixations[fixations['stop_word']==0]
            fixations = fixations[~fixations['surprisal'].isna()]
            fixations = fixations[~fixations['word_freq'].isna()]
            fixations = fixations[~fixations['relative_word_position'].isna()]

            ##### MW 
            fixations['MW'] = fixations['MW'].fillna(-1).astype(int) # third condition coded as -1 for unknown
            # expand to dummy coded cols, one for MW=0 one for MW=1 (intercept is MW=-1)
            fixations['MW=0'] = (fixations['MW']==0).astype(int)
            fixations['MW=1'] = (fixations['MW']==1).astype(int)

            ##### Refixations: detect these as repeated fixations on same page on same IA_ID
            fixations['REFIXATION'] = fixations.duplicated(subset=['IA_ID','identifier'], keep='first')
            # expand to dummy coded cols, one for refixation, one for no refixation
            fixations['REFIXATION=0'] = (fixations['REFIXATION']==0).astype(int)
            fixations['REFIXATION=1'] = (fixations['REFIXATION']==1).astype(int)
            # interaction terms
            fixations['MW=0_REFIXATION=0'] = fixations['MW=0']*fixations['REFIXATION=0']
            fixations['MW=0_REFIXATION=1'] = fixations['MW=0']*fixations['REFIXATION=1']
            fixations['MW=1_REFIXATION=0'] = fixations['MW=1']*fixations['REFIXATION=0']
            fixations['MW=1_REFIXATION=1'] = fixations['MW=1']*fixations['REFIXATION=1']

            ##### covariates
            #  use latencies in trl to look up covariates in events
            fixations = fixations.set_index('eeg_sample')
            lexical_covariates = ['surprisal', 'log_word_freq', 'relative_word_position']
            gaze_covariates = ['INBOUND_SAC_AMPLITUDE']
            cognitive_covariates = ['MW=0','MW=1',
                        # 'REFIXATION=0', # intercept is refixation=0 so not needed
                        'REFIXATION=1',
                        # 'MW=0_REFIXATION=0', # covered by MW=0 as refixation=0 is intercept
                        'MW=0_REFIXATION=1',
                        # 'MW=1_REFIXATION=0', # covered by MW=1 as refixation=0 is intercept
                        'MW=1_REFIXATION=1'
            ]

            ##### covariates
            # separate lexical covariates for MW and no MW fixations so we can look at interaction
            lexical_covariates_2 =[]
            for l in lexical_covariates:
                for c in cognitive_covariates:
                    fixations[f'{l}_{c}'] = fixations[l]*fixations[c]
                    lexical_covariates_2.append(f'{l}_{c}')
            lexical_covariates = lexical_covariates + lexical_covariates_2
            ##### add blinks 
            blinks['task+type'] = 'Blink'
            sham_fixations['task+type'] = 'sham/Fixation'
            blinks.set_index('eeg_sample', inplace=True)
            task_events.set_index('eeg_sample', inplace=True)
            sham_fixations.set_index('eeg_sample', inplace=True)
            # concatenate covariate columns to fixations + task events
            events_to_model = pd.concat([fixations, sham_fixations, blinks, task_events])
            
            # take eeg_sample as a normal column again
            events_to_model.reset_index(inplace=True)
            # drop  duplicates on eeg sample taking into account decimation - GLM complains otherwise
            events_to_model['eeg_sample_decim'] = (events_to_model['eeg_sample']/decimation).astype(int)
            print(f'length before drop duplicates: {len(events_to_model)}')
            events_to_model = events_to_model.drop_duplicates(subset=['eeg_sample_decim'])
            print(f'length after drop duplicates: {len(events_to_model)}')
                    # get covariates back oput of events now dupes have been droped
            covariates = events_to_model.loc[:,['eeg_sample']+lexical_covariates+gaze_covariates+cognitive_covariates]
            # scale and center copntinuous covariates (lex, gaze) but leave MW/refixations as is
            covariates.loc[:,lexical_covariates+gaze_covariates] = covariates.loc[:,lexical_covariates+gaze_covariates].apply(lambda x: (x-x.mean())/x.std(), axis=0)

            # fillna with 0
            covariates = covariates.fillna(0)
            # trick to get trl and trldict 
            annot_to_model = mne.Annotations(onset=events_to_model['latency_sec'], duration=events_to_model['duration_sec'], description=events_to_model['task+type'])
            EEG.set_annotations(annot_to_model)
            trl_to_model, trldict_to_model = mne.events_from_annotations(EEG, regexp='.*Fixation|.*Blink|.*ButtonPress')
            # check covariates same length as trl 
            if len(covariates) != len(trl_to_model):
                print(f'{pID} covariates and trl are different lengths')
                # get matching eeg_samples from trl first column and coviarates eeg_sample
                trl_eeg_samples = trl_to_model[:,0]
                cov_eeg_samples = covariates['eeg_sample']
                common = np.intersect1d(trl_eeg_samples, cov_eeg_samples)
                covariates = covariates[covariates['eeg_sample'].isin(common)]
                trl_to_model = trl_to_model[np.isin(trl_to_model[:,0], common)]
            # check if covariates is singular
            if np.linalg.matrix_rank(covariates) < covariates.shape[1]:
                print(f'{pID} covariates are singular')
                skip_reasons[pID] = 'singular covariates'
                continue

            # count fixations per categorical condition
            cond_counts = {}
            for cond in ['MW=0_REFIXATION=0','MW=1_REFIXATION=0','MW=0_REFIXATION=1','MW=1_REFIXATION=1']:
                cond_counts[cond] = (events_to_model[cond]).sum()
                if cond_counts[cond] < 60:
                    print(f'{pID} has fewer than 60 fixations in condition {cond}')
                    skip_reasons[pID] = 'fewer than 60 fixations per condition'
            cond_counts['FRP'] = (events_to_model['task+type'].str.contains('Fixation').sum())
            cond_counts['pID'] = pID
            cond_counts_all.append(cond_counts)

            ##### model
            tmin, tmax = -0.3, 0.8
            loglevelwas = mne.set_log_level('WARNING', return_old_level=True)
            try:
                X, rERP, stats = ridge_regression_raw(
                    EEG, 
                    events=trl_to_model, event_id=trldict_to_model,
                    tmin=tmin, tmax=tmax, 
                    reject={'eeg': 120e-6}, # uV, as in DImigen Ehinger 2019
                    tstep=1,
                    decim=decimation, #TODO: replace w 1 when finalized
                    covariates=covariates.drop(columns=['eeg_sample']), 
                    model = partial(ridge_model, alpha=ridge_alpha),
                    estimate_stats=False
                )
            except Exception as e:
                print(f'Error in fitting model for {pID}: {e}')
                skip_reasons[pID] = 'model fitting error: ' + str(e)
                continue
            ##### save stats
            save_fn = os.path.join(dir_out, f'{pID}_rERP_stats.npz')
            np.savez(save_fn, **stats)
            rERP['FRP_REFIXATION=1'] = mne.combine_evoked([rERP['reading/Fixation'], rERP['REFIXATION=1']], weights=[1, 1])
            rERP['FRP_MW=0'] = mne.combine_evoked([rERP['MW=0'], rERP['reading/Fixation']], weights=[1, 1])
            rERP['FRP_MW=1'] = mne.combine_evoked([rERP['MW=1'], rERP['reading/Fixation']], weights=[1, 1])
            rERP['FRP_MW=0_REFIXATION=1'] = mne.combine_evoked([rERP['reading/Fixation'], rERP['REFIXATION=1'], rERP['MW=0_REFIXATION=1']], weights=[1, 1, 1])
            rERP['FRP_MW=1_REFIXATION=1'] = mne.combine_evoked([rERP['reading/Fixation'], rERP['REFIXATION=1'], rERP['MW=1_REFIXATION=1']], weights=[1, 1, 1])
        
            # #### visualise significant effects
            # plot p-values on evokeds
            # plotconds = ['reading/Fixation','sham/Fixation','MW=0','MW=1','REFIXATION=1','surprisal','log_word_freq','relative_word_position']
            # fig, ax = plt.subplots(len(plotconds),1, figsize=(12, 12))
            # for i,cond in enumerate(plotconds):
            #     evk = rERP[cond]
            #     condname = evk.comment
            #     pvals = stats['p-values'][cond]
            #     plot_with_stats(evk, pvals, channel='CPz', ax=ax[i])
            #     # set title of axis to cond
            #     ax[i].set_title(cond)
            # fig.savefig(os.path.join(dir_out, f'{pID}_pvals.png'))

            ##### plot contrast between MW and no MW for main effect 
            fig=rERP['reading/Fixation'].plot_joint()
            fig.savefig(os.path.join(dir_out, f'{pID}_FRP_butterfly.png'))

            plot_conds = [
                ['FRP_MW=0','FRP_MW=1'],
                ['reading/Fixation','FRP_REFIXATION=1'],
                ['surprisal','log_word_freq','relative_word_position'],
                ['surprisal_MW=0','surprisal_MW=1'],
                ['surprisal','surprisal_REFIXATION=1','surprisal_MW=0_REFIXATION=1','surprisal_MW=1_REFIXATION=1'],
            ]
            fig, ax = plt.subplots(len(plot_conds), 1,figsize=(12, 9))
            for i,cc in enumerate(plot_conds):
                plotdict = {k:rERP[k] for k in cc}
                mne.viz.plot_compare_evokeds(plotdict, picks='CPz', axes=ax[i] )
            fig.savefig(os.path.join(dir_out, f'{pID}_FRP_effects.png'))

            ##### save regression erps
            mne.write_evokeds(os.path.join(dir_out, f'{pID}_rERP-evk.fif'), [ev for ev in rERP.values()], overwrite=True)
            # append to list of all subjects
            rERP_ALL.append(rERP)
        # except Exception as e:
        #     print(f'Error in {pID}: {e}')
        #     traceback.print_exc()
        #     continue
# save skip reasons
    pd.Series(skip_reasons).to_csv(os.path.join(dir_out, 'skip_reasons.csv'))


#%% read in already processed data
pIDs = [re.findall(r'EML1_\d{3}', f)[0] for f in os.listdir(dir_out) if f.endswith('_rERP-evk.fif')]
skip_reasons = pd.read_csv(os.path.join(dir_out, 'skip_reasons.csv'), index_col=0).to_dict()['0']

#%% screen subjects comrpehension scores
comp_pat = ['Rote','Inf','Deep','SVT']
comp_nafc = [4, 2, 4, 3]
pIDs_before = pIDs
pIDs_remove_MW = []
pIDs_remove_chance = []
for pID in pIDs:
    beh_df_i = beh_df[beh_df['ParticipantID']==pID]
    comp_scores=[]
    print(f'\n----pID: {pID}----')
    for pat in comp_pat:
        comp = beh_df_i.columns.str.contains(pat)
        comp_score = beh_df_i.loc[:,comp]
        # pivot longer
        comp_score = comp_score.melt(value_name='score').dropna()
        comp_score['nafc'] = comp_nafc[comp_pat.index(pat)]
        comp_scores.append(comp_score)
    comp_scores = pd.concat(comp_scores)
    pvals, res = comprehension_above_chance(comp_scores['score'].astype(int).values, comp_scores['nafc'].values)
    print(f'{pID} combined weighted comprehension p-value: {res.pvalue:.3f}')
    if res.pvalue > 0.05:
        print(f'!!!{pID} comprehension is not different from chance, skipping')
        skip_reasons[pID] = 'comprehension not different from chance'
        pIDs_remove_chance.append(pID)
    # check % MW
    MW = beh_df_i['MW'].mean()
    if MW < 0.2 or MW > 0.8:
        print(f'!!!{pID} has {MW:.0%} MW, skipping')
        skip_reasons[pID] = 'MW outside 20-80%'
        pIDs_remove_MW.append(pID)
    else:
        print(f'{pID} has {MW:.0%} MW')
# pIDs_remove = pIDs_remove_MW + pIDs_remove_chance
# pIDs = [p for p in pIDs if p not in pIDs_remove]
# print(f'\n\n---{len(pIDs)} subjects remaining after removing {len(pIDs_remove_chance)} due to chance performance and {len(pIDs_remove_MW)} imbalanced MW scores---')
# print(f'removed the following subjects: {", ".join(pIDs_remove)}')
skip_reasons = pd.Series(skip_reasons)
skip_reasons.to_csv(os.path.join(dir_out, 'skip_reasons.csv'))

#%% exclusions
SKIP_N = False
SKIP_COMP = False
SKIP_MW = True

# load skip_reasons
if SKIP_N:
    pIDs = [p for p in pIDs if skip_reasons.get(p,'') != 'fewer than 60 fixations per condition (MW*REFIXATION)']
if SKIP_COMP:
    pIDs = [p for p in pIDs if skip_reasons.get(p,'')!= 'comprehension not different from chance']
if SKIP_MW:
    pIDs = [p for p in pIDs if skip_reasons.get(p,'') != 'MW outside 20-80%']
print(f'{len(pIDs)} subjects remaining after skipping')

#%%
rERP_list = []
for pID in pIDs:
    rERP = mne.read_evokeds(os.path.join(dir_out, f'{pID}_rERP-evk.fif'))
    rERP_list.append(rERP)

#%% group averages and stats
rERP_ALL = {}
channels = ['CPz', 'FCz', 'AFF5h', 'AFF6h', 'CCP5h', 'CCP6h', 'PPO9h', 'PPO10h']

cond_combinations = {
    'FRP': ['reading/Fixation'],
    'FRP_MW=0': ['reading/Fixation','MW=0'],
    'FRP_MW=1': ['reading/Fixation','MW=1'], 
    'FRP_REFIXATION=1' : ['reading/Fixation', 'REFIXATION=1'],
    'FRP_MW=0_REFIXATION=1': ['reading/Fixation','REFIXATION=1','MW=0_REFIXATION=1'],
    'FRP_MW=1_REFIXATION=1': ['reading/Fixation','REFIXATION=1','MW=1_REFIXATION=1'],
}
condnames = {}
for s in rERP_list:
    # refformat to dict of conditions
    s={evk.comment:evk for evk in s}
    for cond,c in s.items():
        condnames[cond] = cond
        # baseline correct
        c=c.apply_baseline((-.1, 0))
        # downsample 
        c.resample(100)

        if cond in rERP_ALL:
            rERP_ALL[cond].append(c)
        else:
            rERP_ALL[cond] = [c]
    # combo conditions using mne.combine_evoked
    for cc, clist in cond_combinations.items():
        res = mne.combine_evoked([s[ci] for ci in clist], weights=[1 for ci in clist])
        if cc in rERP_ALL:
            rERP_ALL[cc].append(res)
        else:
            rERP_ALL[cc] = [res]
#%% equalize channels 
for c in rERP_ALL:
    rERP_ALL[c]=mne.equalize_channels(rERP_ALL[c])

# combo conditions have annopying long names
condnames.update({k: ' + '.join([ vi for vi in v]) for k,v in cond_combinations.items()})

# %% t test on contrasts
contrast_conds =  [ 
    'FRP',
    ['FRP_MW=0','FRP_MW=1'],
    ['surprisal_MW=0','surprisal_MW=1'],
    ['log_word_freq_MW=0','log_word_freq_MW=1'],
    ['relative_word_position_MW=0','relative_word_position_MW=1'],
    'surprisal','log_word_freq','relative_word_position',
    'INBOUND_SAC_AMPLITUDE',
    'reading/ButtonPress',
    ['reading/Fixation','sham/Fixation'],
    ['FRP','FRP_REFIXATION=1'],
    ['FRP_MW=0_REFIXATION=1','FRP_MW=1_REFIXATION=1'],
                ]
channels = ['CPz']
# threshold_tfce = dict(start=0, step=0.2)

for cc in contrast_conds:
    if isinstance(cc, str) or len(cc) == 1:
        if isinstance(cc, str):
            cc = [cc]
            contrast_name = cc[0]
        x = [rERP.get_data(picks=channels) for rERP in rERP_ALL[cc[0]]]
        x = np.squeeze(np.array(x))
    elif len(cc) == 2:
        # X is list of array, shape (n_observations, p[, q][, r])
        x1 = np.array([rERP.get_data(picks=channels) for rERP in rERP_ALL[cc[0]]])
        x2 = np.array([rERP.get_data(picks=channels) for rERP in rERP_ALL[cc[1]]])
        x=np.squeeze(x2-x1)
        contrast_name = f'{cc[1]} - {cc[0]}'

    T_obs, clusters, cluster_p_values, H0 = mne.stats.permutation_cluster_1samp_test(
            x, n_permutations=1000, seed=42, out_type='mask')  
    times = rERP_ALL[cc[0]][0].times
    fig, ax = plt.subplots()
    plotdict = {k:rERP_ALL[k] for k in cc}
    ax=plot_cluster(clusters, cluster_p_values,times, ax, tcfe=False)  
    # hold on 
    ax=mne.viz.plot_compare_evokeds(plotdict, picks=channels , axes=ax)
    fig.suptitle(f'Group-level {contrast_name}')
    fig.savefig(os.path.join(dir_out, f"Group_n=({len(rERP_list)})_{contrast_name.replace('/','-')}_{channels[0]}.png"))

# %%
cond_counts_all2 = pd.DataFrame.from_dict(cond_counts_all)
cond_counts_all2.to_csv(os.path.join(dir_out, f'cond_counts_all_n={len(rERP_list)}.csv'))
# %%
